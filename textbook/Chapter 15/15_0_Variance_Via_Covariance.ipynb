{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from datascience import *\n",
    "from prob140 import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Via Covariance #\n",
    "In this chapter we return to random sampling, in particular to the variability in the sum of a random sample. Binomial and hypergeometric random variables are such sums. Means of random samples are easily calculated once we have the sample sum. So it is worth taking some time to understand how sample sums behave.\n",
    "\n",
    "By Chebychev's inequality, the value of a random variable $X$ is most likely to be in the range \"$E(X) \\pm$ a few $SD(X)$\". The measure of spread $SD(X)$ is the root mean squared deviation of $X$ from the mean:\n",
    "\n",
    "$$\n",
    "SD(X) = \\sqrt{E[(X-E(X))^2]}\n",
    "$$\n",
    "\n",
    "Variance is the square of the standard deviation. We know that variance has better computational properties than the SD, so this chapter will focus on ways to find variance. We will use some familiar shorthand:\n",
    "\n",
    "- $\\mu_X = E(X)$\n",
    "- $\\sigma_X = SD(X)$\n",
    "\n",
    "Let $D_X = X - \\mu_X$ denote the deviation of $X$ from its mean. Then\n",
    "\n",
    "$$\n",
    "Var(X) = \\sigma_X^2 = E(D_X^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of a Sum ###\n",
    "Let $X$ and $Y$ be two random variables on the same space, and let $S = X+Y$. Then $E(S) = \\mu_X + \\mu_Y$, and the deviation of $S$ is the sum of the deviations of $X$ and $Y$:\n",
    "\n",
    "$$\n",
    "D_S ~ = ~ S - \\mu_S ~ = ~ X + Y - (\\mu_X + \\mu_Y) ~ = ~ D_X + D_Y\n",
    "$$\n",
    "\n",
    "This gives us some insight into the variance of the sum $S$.\n",
    "\n",
    "\\begin{align*}\n",
    "Var(S) &= E(D_S^2) \\\\\n",
    "&= E[(D_X + D_Y)^2] \\\\\n",
    "&= E(D_X^2) + E(D_Y^2) + 2E(D_XD_Y) \\\\\n",
    "&= Var(X) + Var(Y) + 2E(D_XD_Y)\n",
    "\\end{align*}\n",
    "\n",
    "The first thing to note is that while the expectation of a sum is the sum of the expectations, the calculation above shows that the variance of a sum is in general **not** the sum of the variances. There's an extra term. \n",
    "\n",
    "To calculate the variance of a sum, we have to understand that extra term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance ###\n",
    "The *covariance of $X$ and $Y$*, denoted $Cov(X, Y)$, is the expected product of the deviations of $X$ and $Y$:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) ~ = ~ E(D_XD_Y) ~=~ E[(X - \\mu_X)(Y - \\mu_Y)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will learn how to utilize covariance to find variances of sums. The fundamental calculation is the one we did above; here is the result again, using the language of covariance.\n",
    "\n",
    "$$\n",
    "Var(X+Y) ~ = ~ Var(X) + Var(Y) + 2Cov(X, Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
